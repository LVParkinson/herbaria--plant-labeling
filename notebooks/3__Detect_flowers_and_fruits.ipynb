{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdHZvLyW3_3I"
   },
   "source": [
    "# Detect and analyse images\n",
    "This notebook is part of the _Automated plant stage labelling of herbarium samples in the family *Brassicaceae*_ made at [Propulsion Academy Zurich](https://propulsion.academy/?gclid=Cj0KCQiAwf39BRCCARIsALXWETyIhnHT7bA3VYXXOC415brejc6qYXnX7kEpqJmmJ5d5kAcYgoiLhI4aAmPxEALw_wcB) in collaboration with [ETH Library](https://library.ethz.ch/en/)\n",
    "\n",
    "This notebook shows the process to use the trained model to detect fruits and flowers in herabrium sample pictures. For simplicity you can run the `run_detection.py` script using a command line interface. See the README for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KK_Lxtaq3jmj"
   },
   "outputs": [],
   "source": [
    "# Import modules and packages\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.context(\"fivethirtyeight\")\n",
    "from skimage.io import imsave, imread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkQ-u8ELgwIW"
   },
   "source": [
    "### Directory paths\n",
    "\n",
    "Fill in & test the directory needed for inference and data analysis:\n",
    "\n",
    "- ROOT_DIR is the folder where the Mask_RCNN model is saved together with the settings\n",
    "\n",
    "- PROJECT_DIR (full path) is the folder where the images are located and where the output of the model will be saved\n",
    "\n",
    "- INPUT_DIR_NAME is a subdirectory of the PROJECT_DIR where the images that you want to process are\n",
    "\n",
    "- INPUT_ANNOTATIONS_FILE is the (OPTIONAL) name of the JSON files with the annotations in COCO style format\n",
    "\n",
    "- OUTPUT_DIR_NAME is a subdirectory of the PROJECT_DIR where the output information and (OPTIONALLY) the masks will be saved\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 582,
     "status": "ok",
     "timestamp": 1605798208266,
     "user": {
      "displayName": "Matteo Jucker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj9AZsDDD0OROwrJwgWwUjRWaTu83Jzgisj10GWag=s64",
      "userId": "15343200840689388269"
     },
     "user_tz": -60
    },
    "id": "uAn-XLnhguO7",
    "outputId": "b1194925-dd1d-48e4-96bc-bcab90643d27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/matt/Dropbox/ongoing/dataScience/propulsion/eth-library-lab/data\n"
     ]
    }
   ],
   "source": [
    "# directory of input and output images \n",
    "PROJECT_DIR = os.path.join(\"..\", \"data\")\n",
    "assert os.path.exists(PROJECT_DIR), 'PROJECT_DIR does not exist. Did you forget to read the instructions above? ;)'\n",
    "%cd {PROJECT_DIR}\n",
    "\n",
    "# MASK R CNN dir\n",
    "ROOT_DIR =  os.path.join(\"..\", \"src\", \"Mask_RCNN\")\n",
    "assert os.path.exists(ROOT_DIR), 'ROOT_DIR does not exist. Did you forget to read the instructions above? ;)'\n",
    " \n",
    "INPUT_DIR_NAME= \"test\"\n",
    "INPUT_ANNOTATIONS_FILE = \"test/test.json\"\n",
    "OUTPUT_DIR_NAME = \"OUTPUT_Masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZpwGEz91hrPV"
   },
   "outputs": [],
   "source": [
    "# import mask-r-cnn modules and libraries. \n",
    "# These are the files that make the model work. If not available they can be downloaded \n",
    "# from : https://github.com/akTwelve/Mask_RCNN. The notebvbok will attempt to do it automatically\n",
    "sys.path.append(\"../src\")\n",
    "sys.path.append(ROOT_DIR) \n",
    "\n",
    "import herbaria as hb\n",
    "try:\n",
    "  from mrcnn.config import Config\n",
    "  import mrcnn.utils as utils\n",
    "  from mrcnn import visualize\n",
    "  import mrcnn.model as modellib\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "  print(\"modules for MaskRCNN not found, downloading mask-RCNN from source\")\n",
    "  !git clone https://github.com/akTwelve/Mask_RCNN\n",
    "\n",
    "  sys.path.append(os.path.join(ROOT_DIR, Mask_RCNN))\n",
    "  from mrcnn.config import Config\n",
    "  import mrcnn.utils as utils\n",
    "  from mrcnn import visualize\n",
    "  import mrcnn.model as modellib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mObC8nak4B3Y"
   },
   "source": [
    "Below are the configs to create the model and the dataset. They are based on the main class of the Mask_RCNN `utils.Config()` and are present in the `herbaria.py` module. Here we import them from `herbaria.py` and then add some configs specifically for detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 657,
     "status": "ok",
     "timestamp": 1605798235259,
     "user": {
      "displayName": "Matteo Jucker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj9AZsDDD0OROwrJwgWwUjRWaTu83Jzgisj10GWag=s64",
      "userId": "15343200840689388269"
     },
     "user_tz": -60
    },
    "id": "iFbVZ8Vf3_T_",
    "outputId": "bc4bc874-fa87-42fc-af11-c273204f5d4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet50\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     2\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 2\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                15\n",
      "IMAGE_MIN_DIM                  1024\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.01\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               50\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           M_image_augm\n",
      "NUM_CLASSES                    3\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        500\n",
      "POST_NMS_ROIS_TRAINING         1000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                4\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               1\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = hb.HerbariaConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZeHweR3RMhZ"
   },
   "source": [
    "The following are additional settings for inference, based on the **HerbariaConfig** class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-yog9FBzJ9AK"
   },
   "outputs": [],
   "source": [
    "\n",
    "class InferenceConfig(hb.HerbariaConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    IMAGE_MIN_DIM = 1024\n",
    "    IMAGE_MAX_DIM = 1024\n",
    "    DETECTION_MIN_CONFIDENCE = 0.5\n",
    "    \n",
    "inference_config = InferenceConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9DBrbFWRZ6H"
   },
   "source": [
    "# DATASET creation\n",
    "\n",
    "The HerbariaDataset class holds the procedure for creating a dataset to feed into the model (OPTIONAL: suggested only for model evaluation and training). It can be imported from `herbaria.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ZILpaqv_E4t"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "fpath_test_annotations = os.path.join(PROJECT_DIR,INPUT_ANNOTATIONS_FILE)\n",
    "fldr_path_test_images = os.path.join(PROJECT_DIR, INPUT_DIR_NAME)\n",
    "\n",
    "dataset = HerbariaDataset()\n",
    "\n",
    "dataset.load_data(fpath_test_annotations, fldr_path_test_images)\n",
    "dataset.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRdKnuE96-xU"
   },
   "source": [
    "### Create model\n",
    "\n",
    "Here we first create the model using the HerbariaConfig settings, than we load the model using the trained_weights argument.\n",
    "This is equivalent to the `herbaria.load_trained_model` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11439,
     "status": "ok",
     "timestamp": 1605798291999,
     "user": {
      "displayName": "Matteo Jucker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj9AZsDDD0OROwrJwgWwUjRWaTu83Jzgisj10GWag=s64",
      "userId": "15343200840689388269"
     },
     "user_tz": -60
    },
    "id": "npswYNKbUexx",
    "outputId": "56c185ba-e4f8-4aff-df36-582cfb148675"
   },
   "outputs": [],
   "source": [
    "# load_trained_model function\n",
    "def load_trained_model(model_path, mode=\"inference\", config=None):\n",
    "  \"\"\"function to load a model with pre_trained weights.\n",
    "  Params:\n",
    "  - model_path : STRING path saved weights\n",
    "  - mode : STRING \"inference\" or \"train\" DEFAULT \"inference\"\n",
    "  - config :  object of class Config, defaults to InferenceConfig for inference and HerbariaConfig for training\n",
    "  Returns a pre-trained model ready for training or inference\"\"\"\n",
    "  \n",
    "  assert os.path.exists(model_path), \"Provide path to trained weights\"\n",
    "\n",
    "  if config == None:\n",
    "    if mode == 'train':\n",
    "      config = HerbariaConfig()\n",
    "    elif mode == 'inference':\n",
    "      config = InferenceConfig()\n",
    "    else :     \n",
    "        assert \" 'mode' should be 'train' or 'inference' \"\n",
    "\n",
    "  print(\"Loading weights from \", model_path)\n",
    "  model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "  # load weights\n",
    "  model.load_weights(model_path, by_name=True)\n",
    "  return model\n",
    "  \n",
    "model_path = \"../logs/Mask_RCNN/m_image_augm20201116T0937/mask_rcnn_m_image_augm_0009.h5\"\n",
    "model=load_trained_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from ../src/model_weights/m_image_augm20201116T0937/mask_rcnn_m_image_augm_0009.h5\n",
      "Loading weights from  ../src/model_weights/m_image_augm20201116T0937/mask_rcnn_m_image_augm_0009.h5\n",
      "WARNING:tensorflow:From /home/matt/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:574: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "Re-starting from epoch 9\n"
     ]
    }
   ],
   "source": [
    "model = hb.load_trained_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fkd8O4sX8NRW"
   },
   "source": [
    "## Load images and run inference using the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgy9PonDrWIw"
   },
   "source": [
    "### Settings for inference\n",
    "\n",
    "The function `detect_flowers_fruits()` allows to elaborate images and save the result in a systematic manner. It can be accessed through the `run_detection.py` script using the command line interface. See the README for mmore details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_flowers_fruits(input_images_dir, output_folder, model=None, **kwargs):\n",
    "    \"\"\" Runs predictions on images and save the results in output folder.\n",
    "    Optionally can:\n",
    "         Filter the outputs based on the score (confidence of the predictions),\n",
    "         Save masks as png, Display images and outputs (not through Command Line interface)\n",
    "\n",
    "    Returns the filename of the images processed correctly and optionally\n",
    "\n",
    "    Params:\n",
    "    -input_images_dir: path to folder of input images\n",
    "    -output_folder: path to output destination folder\n",
    "    -model : model to use to run predictions (predefined if using CLI)\n",
    "\n",
    "    OPTIONAL params:\n",
    "    -save_predictions: BOOL default True\n",
    "    -save masks: BOOL default False\n",
    "    -filter_scores : REAL between 0 and 1 default 0.5\n",
    "    -save stats : BOOL default True saves a csv of detection statistics for each image\"\"\"\n",
    "\n",
    "    save_predictions = kwargs.get(\"save_predictions\", True)\n",
    "    save_masks = kwargs.get(\"save_masks\", False)\n",
    "    filter_scores = kwargs.get(\"filter_scores\", 0.5, )\n",
    "    show_predictions = kwargs.get(\"show_predictions\", False)\n",
    "    save_stats = kwargs.get(\"save_stats\", True)\n",
    "    labels = kwargs.get(\"labels\", ['flowers', 'fruits'])\n",
    "\n",
    "    print(f\" running inference with the following options:\")\n",
    "    print(f\" image source : \", input_images_dir)\n",
    "    print(f\" destination folder: {output_folder}\")\n",
    "    for k, v in kwargs.items():\n",
    "        print(k, v)\n",
    "\n",
    "    if not model:\n",
    "        model = hb.load_trained_model()\n",
    "\n",
    "    # checks\n",
    "    assert os.listdir(input_images_dir), \"input folder is empty. Please check path or extension of files\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        'dest folder does not exist. It will be created ;)'\n",
    "        os.mkdir(output_folder)\n",
    "\n",
    "    processed_list = []  # list of processed images\n",
    "    stats_list = []\n",
    "    for filename in os.listdir(input_images_dir)[:2]:\n",
    "        if os.path.splitext(filename)[1].lower() in ['.png', '.jpg', '.jpeg']:\n",
    "            image_path = os.path.join(input_images_dir, filename)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        img = skimage.io.imread(image_path)\n",
    "        img_arr = np.array(img)\n",
    "        results = model.detect([img_arr], verbose=0)  # this is equivalent to .predict() and actually does the inference\n",
    "        # filter by scores\n",
    "\n",
    "        r = results[0]  # this contains all the predictions including the masks for each image\n",
    "        # filter results by confidence score\n",
    "        if filter_scores > 0:\n",
    "            print(f\"removing predictions with confidence below {filter_scores}\")\n",
    "\n",
    "            sc_ = r['scores']\n",
    "            lim_ = len(sc_[r['scores'] > filter_scores]) - 1\n",
    "            r['masks'] = r['masks'][:, :, :lim_]\n",
    "            r['rois'] = r['rois'][:lim_]\n",
    "            r['scores'] = r['scores'][:lim_]\n",
    "            r['class_ids'] = r['class_ids'][:lim_]\n",
    "\n",
    "        # create stats table for output\n",
    "        if save_stats:\n",
    "            # create dictionary with labels\n",
    "            stats_dic = {\n",
    "                'img_id': filename.split(\".\")[0],\n",
    "                'min_confidence': filter_scores,\n",
    "                'max_confidence': r['scores'][0],\n",
    "\n",
    "            }\n",
    "            for i in labels:\n",
    "                stats_dic[str(i)+\"_count\"] = 0\n",
    "                stats_dic[str(i)+\"_area\"] = 0\n",
    "            # add count of flowers and fruits\n",
    "            labs, counts = np.unique(r['class_ids'], return_counts=True)\n",
    "            for n in range(len(labs)):\n",
    "                column_head = labels[labs[n]-1]+\"_count\"\n",
    "                # print(\"column line\", column_head)\n",
    "                stats_dic[column_head] = counts[n]\n",
    "\n",
    "            # add area of flowers and fruits(from boxes)\n",
    "            area_sum=[0]*len(labels)\n",
    "            for n, box in enumerate(r['rois']):\n",
    "                lab = r['class_ids'][n]\n",
    "                area = (box[2]-box[0])*(box[3]-box[1])\n",
    "                area_sum[lab-1] = area_sum[lab-1]+area\n",
    "            column_head = labels[lab-1] + \"_area\"\n",
    "            for n,i in enumerate(labels):\n",
    "                stats_dic[ i + \"_area\"] = area_sum[n]\n",
    "            print(stats_dic)\n",
    "            stats_list.append(stats_dic)\n",
    "\n",
    "        # display images\n",
    "        if show_predictions:\n",
    "            visualize.display_instances(img, r['rois'], r['masks'], r['class_ids'],\n",
    "                                        labels, figsize=(100, 100))\n",
    "\n",
    "        # save images\n",
    "        if save_predictions:\n",
    "            print(f\"\\n saving predictions for image {filename} to {output_folder}\")\n",
    "            # create dictionary ( MASKS are EXCLUDED)\n",
    "            json_pred = {'categories': labels,\n",
    "                         'image': filename.split(\".\")[0],\n",
    "                         'rois': r['rois'].tolist(),\n",
    "                         'labels': r['class_ids'].tolist(),\n",
    "                         'scores': r['scores'].tolist()\n",
    "                         }\n",
    "            processed_list.append(filename)\n",
    "\n",
    "            json_path = os.path.join(output_folder, filename.split(\".\")[0] + \".json\")\n",
    "            with open(os.path.join(json_path), \"w+\") as file:\n",
    "                json.dump(json_pred, file)\n",
    "        # saving masks\n",
    "        if save_masks:\n",
    "            # create a single mask with category values\n",
    "            complete_mask = np.zeros(img_arr.shape[:-1], dtype=np.uint8)\n",
    "            for n in range(r['masks'].shape[2]):\n",
    "                single_mask = r['masks'][:, :, n].astype(np.uint8)\n",
    "                single_mask = single_mask * r['class_ids'][n]\n",
    "                complete_mask = complete_mask + single_mask\n",
    "            # save image\n",
    "            mask_path = os.path.join(output_folder, \"MASK-\" + filename.split(\".\")[0] + \".png\")\n",
    "            imsave(mask_path, complete_mask)\n",
    "            print(f\"\\n saved predicted masks for image {filename} as {mask_path}\")\n",
    "            processed_list.append(filename)\n",
    "    # save stats as csv\n",
    "    if save_stats:\n",
    "        stat_path = os.path.join(output_folder, \"detection_stats.csv\")\n",
    "        keys = stats_list[0].keys()\n",
    "        with open(stat_path, 'w+', newline=\"\")as output_file :\n",
    "            dict_writer = csv.DictWriter(output_file, keys)\n",
    "            dict_writer.writeheader()\n",
    "            dict_writer.writerows(stats_list)\n",
    "    proc_images_n = len(set(processed_list))\n",
    "    print(\"processing finished !!\")\n",
    "    print(f\"{proc_images_n} images elaborated\")\n",
    "    return set(processed_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74CHcaet8ZMp"
   },
   "outputs": [],
   "source": [
    "# Set to true to display all images !! Can be slow with many images\n",
    "show_predictions = True\n",
    "\n",
    "# Set to true if you want to save detction outputs\n",
    "save_predictions = False\n",
    "#Set to true if you want to save the mask as image. This can be memory intensive\n",
    "save_masks = False\n",
    "# Set to true if you want to filter the prediction ba scores. Suggested threshold 0.5\n",
    "filter_scores= True \n",
    "filter_score_thresh=0.5\n",
    "assert filter_score_thresh >0, \"Set filter score above 0\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sncuVzXy71Vq"
   },
   "outputs": [],
   "source": [
    "import skimage\n",
    "input_images = os.path.join(PROJECT_DIR,INPUT_DIR_NAME)\n",
    "output_folder= os.path.join(PROJECT_DIR,OUTPUT_DIR_NAME)\n",
    "\n",
    "fpath_test_annotations = os.path.join(PROJECT_DIR,INPUT_ANNOTATIONS_FILE)\n",
    "fldr_path_test_images = os.path.join(PROJECT_DIR, INPUT_DIR_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1Cd71ShKn6R-RBXzhMaNYssjFCtxN_79p"
    },
    "executionInfo": {
     "elapsed": 45492,
     "status": "ok",
     "timestamp": 1605798426876,
     "user": {
      "displayName": "Matteo Jucker",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj9AZsDDD0OROwrJwgWwUjRWaTu83Jzgisj10GWag=s64",
      "userId": "15343200840689388269"
     },
     "user_tz": -60
    },
    "id": "ZSYQsRLu7v-a",
    "outputId": "e7289d6e-cb73-4eb7-f085-671912027259"
   },
   "outputs": [],
   "source": [
    "detect_flowers_fruits(input_images, output_folder, model, show_predictions=True, save_predictions=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xzLjkXdUkoPs"
   },
   "outputs": [],
   "source": [
    "img_dir = os.path.join(PROJECT_DIR,INPUT_DIR_NAME)\n",
    "gt_json_path= os.path.join(PROJECT_DIR,INPUT_ANNOTATIONS_FILE)\n",
    " \n",
    " # create dataset\n",
    "dataset = HerbariaDataset()\n",
    "dataset.load_data(gt_json_path, img_dir)\n",
    "dataset.prepare()\n",
    "for image_id in dataset.image_ids[:2]:\n",
    "  # load image and annotations from GT and detections\n",
    "  image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "          modellib.load_image_gt(dataset, config,\n",
    "                                  image_id)\n",
    "  results = model.detect([image], verbose=0)\n",
    "  r = results[0]\n",
    "  # display gt\n",
    "  visualize.display_instances(image, gt_bbox, gt_mask, gt_class_id, \n",
    "                                  dataset.class_names,figsize=(100,100), show_mask=False)\n",
    "  # display pred\n",
    "  visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                                  dataset.class_names,figsize=(100,100), show_mask=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ClvBop0CgiV"
   },
   "source": [
    "# Model performance\n",
    "\n",
    "below a selection of functions to evaluate model performance in a more thorough way. More functions are available i9n the original Mask_RCNN repo. See thi readme for more details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftl8Bro4LacS"
   },
   "outputs": [],
   "source": [
    "# Compute VOC-style Average Precision\n",
    "def compute_batch_ap(image_ids):\n",
    "    sp_eval = {}\n",
    "    sp_eval['AP']=[]\n",
    "    sp_eval['precisions']=[]\n",
    "    sp_eval['recalls']=[]\n",
    "    sp_eval['overlaps']=[]\n",
    "\n",
    "    for image_id in image_ids:\n",
    "        # Load image\n",
    "        image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "            modellib.load_image_gt(dataset, config,\n",
    "                                   image_id)\n",
    "        # Run object detection\n",
    "        results = model.detect([image], verbose=0)\n",
    "        # Compute AP\n",
    "        r = results[0]\n",
    "        AP, precisions, recalls, overlaps =\\\n",
    "            utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                              r['rois'], r['class_ids'], r['scores'], r['masks'])\n",
    "        sp_eval['AP'].append(AP)\n",
    "        sp_eval['precisions'].append(precisions)\n",
    "        sp_eval['recalls'].append(recalls)\n",
    "        sp_eval['overlaps'].append(overlaps)\n",
    "    return sp_eval\n",
    "\n",
    "# # Pick a set of random images\n",
    "# image_ids = np.random.choice(dataset.image_ids, 10)\n",
    "# APs = compute_batch_ap(image_ids)\n",
    "# print(\"mAP @ IoU=50: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvdB5GOjXFRt"
   },
   "outputs": [],
   "source": [
    "from utils import compute_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EM95QJvpuJF7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import seaborn as sns\n",
    "sns.color_palette(\"Spectral\", as_cmap=True)\n",
    "def evaluate_model_performance(img_dir, gt_json_path, model, filter_score=0.5, viz=False):\n",
    "  \"\"\" evaluates the model peroformance against a test set of images ad annotations\n",
    "  Parameters:\n",
    "  - model ??\n",
    "  - detections folder with json files as retruned by <detect_flower_fruits> function\n",
    "  - ground truth data (annotations) in COCO format\n",
    "\n",
    "  OPTIONAL Parameters:\n",
    "  -plot outputs\n",
    "  -masks\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  # create dataset\n",
    "  dataset = HerbariaDataset()\n",
    "  dataset.load_data(gt_json_path, img_dir)\n",
    "  dataset.prepare()\n",
    "  \n",
    "  # for det in os.listdir(detections_dir):\n",
    "  #   if det.split(\".\") == \"json\":\n",
    "  n_record=[]\n",
    "  f1_boxes=[]\n",
    "  recall_boxes=[]\n",
    "  for image_id in dataset.image_ids:\n",
    "  # load image and annotations from GT and detections\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "            modellib.load_image_gt(dataset, config,\n",
    "                                   image_id)\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "\n",
    "    if filter_score:\n",
    "        print(f\"removing predictions below {filter_score}\")\n",
    "        sc_=r['scores']\n",
    "        lim_=len(sc_[ sc_ > filter_score ])-1\n",
    "        r['masks']=r['masks'][:,:,:lim_]\n",
    "        r['rois']=r['rois'][:lim_]\n",
    "        r['scores']= r['scores'][:lim_]\n",
    "        r['class_ids'] = r['class_ids'][:lim_]\n",
    "  # boxes classification\n",
    "    max_len =min(len(gt_class_id), len(r['class_ids']))\n",
    "    print(max_len)\n",
    "    f1_=f1_score(gt_class_id[:max_len],r['class_ids'][:max_len])\n",
    "    f1_boxes.append(f1_)\n",
    "    print(\"\\n f1 score : \", f1_)\n",
    "  \n",
    "  # Overlaps boxes and masks\n",
    "  # sp_eval = compute_batch_ap(dataset.image_ids)\n",
    "  # IOU masks\n",
    "  \n",
    "    iou_recall,_=utils.compute_recall(r['rois'], gt_bbox, 0)\n",
    "    print(\"recall boxes:\", iou_recall)\n",
    "    recall_boxes.append(iou_recall)\n",
    "  # break\n",
    "  # compare areas\n",
    "    # break\n",
    "\n",
    "\n",
    "  print(f\" mean f1 score of batch : {np.mean(f1_boxes)}\")\n",
    "  print(f\"mean recall score of batch : {np.mean(recall_boxes)}\")\n",
    "  print(f1_boxes)\n",
    "\n",
    "  if viz:\n",
    "    pass\n",
    "    \n",
    "    \n",
    "  eval_results={\n",
    "      \"f1_score\": f1_boxes, \n",
    "      \"recall_boxes\": recall_boxes\n",
    "                  \n",
    "                  }\n",
    "  return eval_results\n",
    "\n",
    "img_dir = os.path.join(PROJECT_DIR,INPUT_DIR_NAME)\n",
    "gt_json_path= os.path.join(PROJECT_DIR,INPUT_ANNOTATIONS_FILE)\n",
    "perf=evaluate_model_performance(img_dir, gt_json_path, model, 0.5, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wL6AKRUhLctF"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rc('axes',edgecolor=\"#FFFFFF\")\n",
    "matplotlib.rc('lines', color= \"#FFFFFF\")\n",
    "# matplotlib.rc('xticks', color= \"#FFFFFF\")\n",
    "\n",
    "\n",
    "sns.color_palette(\"Spectral\", as_cmap=True)\n",
    "fig, ax = plt.subplots(ncols=1, nrows=2, figsize=(20,20))\n",
    "fig.set_facecolor(\"#000000\")\n",
    "ax[0].hist(perf['f1_score'], bins=10, edgecolor='white', alpha=0.6)\n",
    "# ax[0].set_facecolor(\"#000000\")\n",
    "ax[0].set_title(\"F1 score object classification\")\n",
    "ax[0].axvline(x=np.mean(perf['f1_score']), color=\"blue\")\n",
    "# ax[0].plot(perf['f1_score'], color='blue', alpha=0.6)\n",
    "\n",
    "ax[1].hist(perf['recall_boxes'], bins=10, edgecolor='white', alpha=0.6)\n",
    "ax[1].axvline(x=np.mean(perf['recall_boxes']), color=\"blue\")\n",
    "\n",
    "# ax[1].set_facecolor(\"#000000\")\n",
    "ax[0].xaxis.label.set_color(\"#FFFFFF\")\n",
    "ax[0].tick_params(axis='x', colors=\"#FFFFFF\")\n",
    "ax[1].tick_params(axis='x', colors=\"#FFFFFF\")\n",
    "ax[0].tick_params(axis='y', colors=\"#FFFFFF\")\n",
    "ax[1].tick_params(axis='y', colors=\"#FFFFFF\")\n",
    "ax[0].grid(axis='y',color=\"#000000\", lw=2)\n",
    "ax[1].grid(axis='y',color=\"#000000\", lw=2)\n",
    "\n",
    "plt.savefig('eval1.pdf')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4x-wx5UA1fM"
   },
   "source": [
    "# Image analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxEV8jSgCMJk"
   },
   "outputs": [],
   "source": [
    "perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbBoBco2uU7q"
   },
   "outputs": [],
   "source": [
    "def analyse_batch_detections(image_folder, json_folder, mask_folder,):\n",
    "  \"\"\"\" function to analys images AFTER detection. \n",
    "  Returns dictionary with data about prediction batch  \n",
    "  Parameters:\n",
    "  json_folder:  folder with the output of the model as produced by <detect_flower_fruits> function\n",
    "\n",
    "  OPTIONAL parameters:\n",
    "  mask_folder\n",
    "  plot\n",
    "  \"\"\"\n",
    "  # load image and data\n",
    "  \n",
    "  # flowers\n",
    "\n",
    "  # image classification\n",
    "\n",
    "  # flowers per image mean and stdev\n",
    "\n",
    "  # fruits per image mean and standard dev\n",
    "\n",
    "  pass\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Detect_flowers_and_fruits.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
