{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Detect_flowers_and_fruits.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fdHZvLyW3_3I"},"source":["# Detect and recognize images\n","\n","This notebook shows the process to use the trained model to detect fruits and flowers in herabrium sample pictures. For simplicity you can run the detect_herbaria script using a command line interface:\n","\n","TODO: ADD example"]},{"cell_type":"code","metadata":{"id":"KK_Lxtaq3jmj"},"source":["# Import modules and packages\n","import os\n","import sys\n","import json\n","import numpy as np\n","import time\n","from PIL import Image, ImageDraw\n","import matplotlib.pyplot as plt\n","plt.style.context(\"fivethirtyeight\")\n","from skimage.io import imsave, imread\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r1aalOU5IB1H","executionInfo":{"status":"ok","timestamp":1605798203228,"user_tz":-60,"elapsed":1582,"user":{"displayName":"Matteo Jucker","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj9AZsDDD0OROwrJwgWwUjRWaTu83Jzgisj10GWag=s64","userId":"15343200840689388269"}},"outputId":"1097548a-35ad-4af0-c38e-fa545fe40804"},"source":["# connect to gdrive # TO BE REMOVED\n","from google.colab import drive\n","drive.flush_and_unmount()\n","drive.mount('/gdrive', force_remount=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VkQ-u8ELgwIW"},"source":["### Directory paths\n","\n","Fill in & test the directory needed for inference and data analysis:\n","\n","- ROOT_DIR is the folder where the Mask_RCNN model is saved together with the settings\n","\n","- PROJECT_DIR (full path) is the folder where the images are located and where the output of the model will be saved\n","\n","- INPUT_DIR_NAME is a subdirectory of the PROJECT_DIR where the images that you want to process are\n","\n","- INPUT_ANNOTATIONS_FILE is the (OPTIONAL) name of the JSON files with the annotations in COCO style format\n","\n","- OUTPUT_DIR_NAME is a subdirectory of the PROJECT_DIR where the output information and (OPTIONALLY) the masks will be saved\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"uAn-XLnhguO7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605798208266,"user_tz":-60,"elapsed":582,"user":{"displayName":"Matteo Jucker","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj9AZsDDD0OROwrJwgWwUjRWaTu83Jzgisj10GWag=s64","userId":"15343200840689388269"}},"outputId":"b1194925-dd1d-48e4-96bc-bcab90643d27"},"source":["# directory of input and output images \n","PROJECT_DIR = \"/gdrive/My Drive/brassicacea_automatic_classification/coding/ETH_DATASET\"\n","assert os.path.exists(PROJECT_DIR), 'PROJECT_DIR does not exist. Did you forget to read the instructions above? ;)'\n","%cd {PROJECT_DIR}\n","\n","# MASK R CNN dir\n","ROOT_DIR =  '/gdrive/My Drive/brassicacea_automatic_classification/coding/herbaria--plant-labeling/src/Mask_RCNN'\n","assert os.path.exists(ROOT_DIR), 'ROOT_DIR does not exist. Did you forget to read the instructions above? ;)'\n"," \n","\n","INPUT_DIR_NAME= \"preproc/test\"\n","INPUT_ANNOTATIONS_FILE = \"preproc/test/test.json\"\n","OUTPUT_DIR_NAME = \"OUTPUT_images_MASK_R_CNN\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/gdrive/My Drive/brassicacea_automatic_classification/coding/ETH_DATASET\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":333},"id":"qerzZGmu_Gqa","executionInfo":{"status":"error","timestamp":1605798131435,"user_tz":-60,"elapsed":1093,"user":{"displayName":"Matteo Jucker","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj9AZsDDD0OROwrJwgWwUjRWaTu83Jzgisj10GWag=s64","userId":"15343200840689388269"}},"outputId":"1519e456-16b3-4445-f813-e3f07a3b46d6"},"source":["# import custom modules from herbaria.py\n","#PROBALY after is sys.path.append(\"../src\")\n","# import brassicas as bs\n","import sys\n","path=os.path.join( os.path.abspath(os.path.join(ROOT_DIR, os.pardir)))\n","sys.path.append(path)\n","import herbaria\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-fc9f72ce5ead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpardir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mherbaria\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/gdrive/My Drive/brassicacea_automatic_classification/coding/herbaria--plant-labeling/src/herbaria.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mROOT_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./Mask_RCNN\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Mask_RCNN module not found.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_DIR\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# To find local version of the library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: Mask_RCNN module not found."]}]},{"cell_type":"code","metadata":{"id":"ZpwGEz91hrPV"},"source":["\n","import helper_functions as hp\n","\n","# import mask-r-cnn modules and libraries. \n","# These are the files that make the model work. If not available they can be downloaded \n","# from : https://github.com/akTwelve/Mask_RCNN. The notebvbok will attempt to do it automatically\n","\n","sys.path.append(ROOT_DIR) \n","try:\n","  from mrcnn.config import Config\n","  import mrcnn.utils as utils\n","  from mrcnn import visualize\n","  import mrcnn.model as modellib\n","\n","except ModuleNotFoundError:\n","  print(\"modules for MaskRCNN not found, downloading mask-RCNN from source\")\n","  !git clone https://github.com/akTwelve/Mask_RCNN\n","\n","  sys.path.append(os.path.join(ROOT_DIR, Mask_RCNN))\n","  from mrcnn.config import Config\n","  import mrcnn.utils as utils\n","  from mrcnn import visualize\n","  import mrcnn.model as modellib\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mObC8nak4B3Y"},"source":["Below are the configs to create the model and the dataset. They are based on the main class of the Mask_RCNN utils.Config() and are present in the herbaria.py module. Here we show the full config. Then we actually import it from herbaria.py\n"]},{"cell_type":"code","metadata":{"id":"iFbVZ8Vf3_T_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605798235259,"user_tz":-60,"elapsed":657,"user":{"displayName":"Matteo Jucker","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj9AZsDDD0OROwrJwgWwUjRWaTu83Jzgisj10GWag=s64","userId":"15343200840689388269"}},"outputId":"bc4bc874-fa87-42fc-af11-c273204f5d4f"},"source":["class HerbariaConfig(Config):\n","    \"\"\"Configuration for training on the cigarette butts dataset.\n","    Derives from the base Config class and overrides values specific\n","    to the cigarette butts dataset.\n","    \"\"\"\n","    # Give the configuration a recognizable name\n","    NAME = \"M_image_augm\"\n","\n","    # Train on 1 GPU and 1 image per GPU. Batch size is 1 (GPUs * images/GPU).\n","    GPU_COUNT = 1\n","    IMAGES_PER_GPU = 2\n","\n","    # Number of classes (including background)\n","    NUM_CLASSES = 1 + 2   # background + 2 [ 'flower', fruit]\n","\n","    # All of our training images are 1024x1024\n","    IMAGE_MIN_DIM = 1024\n","    IMAGE_MAX_DIM = 1024\n","\n","    # You can experiment with this number to see if it improves training\n","    STEPS_PER_EPOCH = 4\n","\n","    # This is how often validation is run. If you are using too much hard drive space\n","    # on saved models (in the MODEL_DIR), try making this value larger.\n","    VALIDATION_STEPS = 1\n","    \n","    # Matterport originally used resnet101, but I downsized to fit it on my graphics card\n","    BACKBONE = 'resnet50'\n","\n","    # To be honest, I haven't taken the time to figure out what these do\n","    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n","    TRAIN_ROIS_PER_IMAGE = 32\n","    MAX_GT_INSTANCES = 50 \n","    POST_NMS_ROIS_INFERENCE = 500 \n","    POST_NMS_ROIS_TRAINING = 1000 \n","    LEARNING_RATE=0.01\n","    \n","config = HerbariaConfig()\n","config.display()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Configurations:\n","BACKBONE                       resnet50\n","BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n","BATCH_SIZE                     2\n","BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n","COMPUTE_BACKBONE_SHAPE         None\n","DETECTION_MAX_INSTANCES        100\n","DETECTION_MIN_CONFIDENCE       0.7\n","DETECTION_NMS_THRESHOLD        0.3\n","FPN_CLASSIF_FC_LAYERS_SIZE     1024\n","GPU_COUNT                      1\n","GRADIENT_CLIP_NORM             5.0\n","IMAGES_PER_GPU                 2\n","IMAGE_CHANNEL_COUNT            3\n","IMAGE_MAX_DIM                  1024\n","IMAGE_META_SIZE                15\n","IMAGE_MIN_DIM                  1024\n","IMAGE_MIN_SCALE                0\n","IMAGE_RESIZE_MODE              square\n","IMAGE_SHAPE                    [1024 1024    3]\n","LEARNING_MOMENTUM              0.9\n","LEARNING_RATE                  0.01\n","LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n","MASK_POOL_SIZE                 14\n","MASK_SHAPE                     [28, 28]\n","MAX_GT_INSTANCES               50\n","MEAN_PIXEL                     [123.7 116.8 103.9]\n","MINI_MASK_SHAPE                (56, 56)\n","NAME                           M_image_augm\n","NUM_CLASSES                    3\n","POOL_SIZE                      7\n","POST_NMS_ROIS_INFERENCE        500\n","POST_NMS_ROIS_TRAINING         1000\n","PRE_NMS_LIMIT                  6000\n","ROI_POSITIVE_RATIO             0.33\n","RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n","RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n","RPN_ANCHOR_STRIDE              1\n","RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n","RPN_NMS_THRESHOLD              0.7\n","RPN_TRAIN_ANCHORS_PER_IMAGE    256\n","STEPS_PER_EPOCH                4\n","TOP_DOWN_PYRAMID_SIZE          256\n","TRAIN_BN                       False\n","TRAIN_ROIS_PER_IMAGE           32\n","USE_MINI_MASK                  True\n","USE_RPN_ROIS                   True\n","VALIDATION_STEPS               1\n","WEIGHT_DECAY                   0.0001\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bm_2ct94J96_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jZeHweR3RMhZ"},"source":["The following are additional settings for inference, based on the **HerbariaConfig** class"]},{"cell_type":"code","metadata":{"id":"-yog9FBzJ9AK"},"source":["\n","class InferenceConfig(HerbariaConfig):\n","    GPU_COUNT = 1\n","    IMAGES_PER_GPU = 1\n","    IMAGE_MIN_DIM = 1024\n","    IMAGE_MAX_DIM = 1024\n","    DETECTION_MIN_CONFIDENCE = 0.5\n","    \n","inference_config = InferenceConfig()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g9DBrbFWRZ6H"},"source":["# DATASET creation\n","\n","The HerbariaDataset class holds the procedure for creating a dataset to feed into the model (OPTIONAL: suggested only for model evaluation and training). It can be imported from herbaria.py as well with the call\n","`from herbaria import HerbariaDataset`\n"]},{"cell_type":"code","metadata":{"id":"dPOOPbAL5WrU"},"source":["class HerbariaDataset(utils.Dataset):\n","    \"\"\" Generates a COCO-like dataset, i.e. an image dataset annotated in the style of the COCO dataset.\n","        See http://cocodataset.org/#home for more information.\n","    \"\"\"\n","    def load_data(self, annotation_json, images_dir):\n","        \"\"\" Load the coco-like dataset from json\n","        Args:\n","            annotation_json: The path to the coco annotations json file\n","            images_dir: The directory holding the images referred to by the json file\n","        \"\"\"\n","        # Load json from file\n","        json_file = open(annotation_json)\n","        coco_json = json.load(json_file)\n","        json_file.close()\n","        \n","        # Add the class names using the base method from utils.Dataset\n","        source_name = \"coco_like\"\n","        for category in coco_json['categories']:\n","            class_id = category['id']\n","            class_name = category['name']\n","            if class_id < 1:\n","                print('Error: Class id for \"{}\" cannot be less than one. (0 is reserved for the background)'.format(class_name))\n","                return\n","            \n","            self.add_class(source_name, class_id, class_name)\n","        \n","        # Get all annotations\n","        annotations = {}\n","        for annotation in coco_json['annotations']:\n","            image_id = annotation['image_id']\n","            if image_id not in annotations:\n","                annotations[image_id] = []\n","            annotations[image_id].append(annotation)\n","        \n","        # Get all images and add them to the dataset\n","        seen_images = {}\n","        for image in coco_json['images']:\n","            image_id = image['id']\n","            if image_id in seen_images:\n","                print(\"Warning: Skipping duplicate image id: {}\".format(image))\n","            else:\n","                seen_images[image_id] = image\n","                try:\n","                    image_file_name = image['file_name']\n","                    image_width = image['width']\n","                    image_height = image['height']\n","                except KeyError as key:\n","                    print(\"Warning: Skipping image (id: {}) with missing key: {}\".format(image_id, key))\n","                \n","                image_path = os.path.abspath(os.path.join(images_dir, image_file_name))\n","                image_annotations = annotations[image_id]\n","                \n","                # Add the image using the base method from utils.Dataset\n","                self.add_image(\n","                    source=source_name,\n","                    image_id=image_id,\n","                    path=image_path,\n","                    width=image_width,\n","                    height=image_height,\n","                    annotations=image_annotations\n","                )\n","                \n","    def load_mask(self, image_id):\n","        \"\"\" Load instance masks for the given image.\n","        MaskRCNN expects masks in the form of a bitmap [height, width, instances].\n","        Args:\n","            image_id: The id of the image to load masks for\n","        Returns:\n","            masks: A bool array of shape [height, width, instance count] with\n","                one mask per instance.\n","            class_ids: a 1D array of class IDs of the instance masks.\n","        \"\"\"\n","        image_info = self.image_info[image_id]\n","        annotations = image_info['annotations']\n","        instance_masks = []\n","        class_ids = []\n","        \n","        for annotation in annotations:\n","            class_id = annotation['category_id']\n","            mask = Image.new('1', (image_info['width'], image_info['height']))\n","            mask_draw = ImageDraw.ImageDraw(mask, '1')\n","            for segmentation in annotation['segmentation']:\n","                mask_draw.polygon(segmentation, fill=1)\n","                bool_array = np.array(mask) > 0\n","                instance_masks.append(bool_array)\n","                class_ids.append(class_id)\n","\n","        mask = np.dstack(instance_masks)\n","        class_ids = np.array(class_ids, dtype=np.int32)\n","        \n","        return mask, class_ids"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y8cb429r52WD"},"source":["### Folders and model settings"]},{"cell_type":"code","metadata":{"id":"Gx7p3CcP30sM"},"source":["# Set the ROOT_DIR variable to the root directory of the Mask_RCNN model\n","\n","assert os.path.exists(ROOT_DIR), 'ROOT_DIR does not exist. Did you forget to read the instructions above? ;)'\n","\n","# Set the data sources for images and OPTIONAL annotations /ground truth (for testing set)\n","fpath_test_annotations = os.path.join(PROJECT_DIR,INPUT_ANNOTATIONS_FILE)\n","fldr_path_test_images = os.path.join(PROJECT_DIR, INPUT_DIR_NAME)\n","\n","# Folder for the model weights\n","MODEL_DIR = os.path.join(\"logs/Mask_RCNN\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oOiXoC-j61nS"},"source":["### Load data\n"]},{"cell_type":"code","metadata":{"id":"U7poQV5n6bCO"},"source":["# ROOT_DIR =  'Mask_RCNN'\n","# # Import mrcnn libraries\n","# sys.path.append(ROOT_DIR) \n","# from mrcnn.config import Config\n","# import mrcnn.utils as utils\n","# from mrcnn import visualize\n","# import mrcnn.model as modellib"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ZILpaqv_E4t"},"source":["## THIS COULD BE REMOVED PROBABLY\n","# Load dataset\n","dataset = HerbariaDataset()\n","\n","dataset.load_data(fpath_test_annotations, fldr_path_test_images)\n","dataset.prepare()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xRdKnuE96-xU"},"source":["### Create model\n","\n","Here we first create the model using the HerbariaConfig settings, than we load the model using the trained_weights argument.\n","This is equivalent to the `herbaria.load_trained_model` function"]},{"cell_type":"code","metadata":{"id":"-USEDfII5w5I"},"source":["# Recreate the model in inference mode\n","model = modellib.MaskRCNN(mode=\"inference\", \n","                          config=inference_config,\n","                          model_dir=MODEL_DIR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4jp6lGZ17UXB"},"source":["# Get path to saved weights\n","# Either set a specific path or find last trained weights\n","# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n","# model_path = model.find_last()\n","model_path = \"../logs/Mask_RCNN/m_image_augm20201116T0937/mask_rcnn_m_image_augm_0009.h5\"\n","\n","# Load trained weights (fill in path to trained weights here)\n","assert model_path != \"\", \"Provide path to trained weights\"\n","print(\"Loading weights from \", model_path)\n","model.load_weights(model_path, by_name=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"npswYNKbUexx","executionInfo":{"status":"ok","timestamp":1605798291999,"user_tz":-60,"elapsed":11439,"user":{"displayName":"Matteo Jucker","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj9AZsDDD0OROwrJwgWwUjRWaTu83Jzgisj10GWag=s64","userId":"15343200840689388269"}},"outputId":"56c185ba-e4f8-4aff-df36-582cfb148675"},"source":["# load_trained_model function\n","def load_trained_model(model_path, mode=\"inference\", config=None):\n","  \"\"\"function to load a model with pre_trained weights.\n","  Params:\n","  - model_path : STRING path saved weights\n","  - mode : STRING \"inference\" or \"train\" DEFAULT \"inference\"\n","  - config :  object of class Config, defaults to InferenceConfig for inference and HerbariaConfig for training\n","  Returns a pre-trained model ready for training or inference\"\"\"\n","  \n","  assert os.path.exists(model_path), \"Provide path to trained weights\"\n","\n","  if config == None:\n","    if mode == 'train':\n","      config = HerbariaConfig()\n","    elif mode == 'inference':\n","      config = InferenceConfig()\n","    else :     \n","        assert \" 'mode' should be 'train' or 'inference' \"\n","\n","  print(\"Loading weights from \", model_path)\n","  model = modellib.MaskRCNN(mode=\"inference\", \n","                          config=inference_config,\n","                          model_dir=MODEL_DIR)\n","  # load weights\n","  model.load_weights(model_path, by_name=True)\n","  return model\n","  \n","model_path = \"../logs/Mask_RCNN/m_image_augm20201116T0937/mask_rcnn_m_image_augm_0009.h5\"\n","model=load_trained_model(model_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading weights from  ../logs/Mask_RCNN/m_image_augm20201116T0937/mask_rcnn_m_image_augm_0009.h5\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:574: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use fn_output_signature instead\n","Re-starting from epoch 9\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Fkd8O4sX8NRW"},"source":["## Load images and run inference using the model\n"]},{"cell_type":"code","metadata":{"id":"WML4Y-mayH7b"},"source":["# todo ADD THRESHOLD FOR ACCEPTABLE PREDICTION HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jgy9PonDrWIw"},"source":["### Settings for inference\n","\n","The following are the settings to run the inference"]},{"cell_type":"code","metadata":{"id":"74CHcaet8ZMp"},"source":["# Set to true to display all images !! Can be slow with many images\n","show_predictions = True\n","\n","# Set to true if you want to save detction outputs\n","save_predictions = False\n","#Set to true if you want to save the mask as image. This can be memory intensive\n","save_masks = False\n","# Set to true if you want to filter the prediction ba scores. Suggested threshold 0.5\n","filter_scores= True \n","filter_score_thresh=0.5\n","assert filter_score_thresh >0, \"Set filter score above 0\"\n","\n","\n","\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sncuVzXy71Vq"},"source":["import skimage\n","input_images = os.path.join(PROJECT_DIR,INPUT_DIR_NAME)\n","output_folder= os.path.join(PROJECT_DIR,OUTPUT_DIR_NAME)\n","\n","def detect_flowers_fruits(input_images_dir, output_folder, model, **kwargs):\n","  \"\"\" Runs predictions on images and save output in folder. \n","  Optionally can:\n","       Filter the outputs based on the score (confidence of the predictions),\n","       Save masks as png,\n","       Display images and outputs  \n","\n","  Returns the filename of the images processed correctly\n","\n","  Params:\n","  -input_images_dir: abs path to folder of input images\n","  -output_folder: abs path to output destination folder\n","  -model : model to use to run predictions\n","  OPTIONAL params:\n","  -save_predictions: BOOL default True  \n","  -save masks: BOOL default False\n","  -filter_scores: BOOL default True\n","  -filter_score_thresh: Decimal between 0 and 1 default 0.5\"\"\"\n","\n","\n","  save_predictions=kwargs.get(\"save_predictions\", True)\n","  save_masks=kwargs.get(\"save_masks\", False)\n","  filter_scores=kwargs.get(\"filter_scores\", True, )\n","  filter_score_thresh=kwargs.get(\"filter_score_thresh\", 0.5, )\n","  show_predictions=kwargs.get(\"show_predictions\", False )\n","\n","  print(f\" running inference with the following options:\")\n","  print(f\" image source : \", input_images_dir)\n","  print(f\" destination folder: {output_folder}\")\n","  for k,v in kwargs.items():\n","    print(k,v)\n","\n","  # checks\n","  assert os.listdir(input_images_dir), \"input folder is empty. Please check path or extension of files\"\n","  if not os.path.exists(output_folder):\n","    'dest folder does not exist. It will be created ;)'\n","    os.mkdir(output_folder) \n","\n","  processed_list=[]\n","  for filename in os.listdir(input_images)[:2]:\n","      if os.path.splitext(filename)[1].lower() in ['.png', '.jpg', '.jpeg']:\n","          image_path=os.path.join(input_images, filename)\n","      else: \n","        continue\n","\n","      img = skimage.io.imread(image_path)\n","      img_arr = np.array(img)\n","      results = model.detect([img_arr], verbose=0) # this is equivalent to .predict() and actually does the inference\n","      # filter by scores\n","      \n","      r = results[0] # this contains all the predictions including the masks for one image\n","      \n","      if filter_scores:\n","        print(f\"removing predictions with confidence below {filter_score_thresh}\")\n","        \n","        sc_=r['scores']\n","        lim_=len(sc_[ r['scores'] > filter_score_thresh ])-1\n","        r['masks']=r['masks'][:,:,:lim_]\n","        r['rois']=r['rois'][:lim_]\n","        r['scores']= r['scores'][:lim_]\n","        r['class_ids'] = r['class_ids'][:lim_]\n","    \n","      # display images\n","      if show_predictions:\n","        visualize.display_instances(img, r['rois'], r['masks'], r['class_ids'], \n","                                  dataset.class_names,figsize=(100,100))\n","      # save images\n","      if save_predictions:\n","        print(f\"\\n saving predictions for image {filename} to {output_folder}\")\n","        # create dictionary ( FOR NOW MASKS are EXCLUDED)\n","        json_pred={}\n","        json_pred['categories']=dataset.class_names     \n","        json_pred['image']=filename.split(\".\")[0]\n","        json_pred['rois']=r['rois'].tolist()\n","        json_pred['labels']=r['class_ids'].tolist()\n","        json_pred['scores']=r['scores'].tolist()\n","        processed_list.append(filename)\n","\n","        json_path=os.path.join(output_folder, filename.split(\".\")[0] + \".json\")\n","        with open(os.path.join(json_path), \"w+\") as file:\n","          json.dump(json_pred, file)\n","      if save_masks:\n","\n","        # create a single mask with category values\n","        complete_mask = np.zeros(img_arr.shape[:-1], dtype=np.uint8)\n","        for n in range(r['masks'].shape[2]):\n","          single_mask = r['masks'][:,:,n].astype(np.uint8)\n","          single_mask = single_mask*r['class_ids'][n]\n","          complete_mask=complete_mask+single_mask\n","        #save image\n","        mask_path=os.path.join(output_folder, \"MASK-\" + filename.split(\".\")[0] + \".png\")\n","        imsave(mask_path, complete_mask)\n","        print(f\"\\n saved predicted masks for image {filename} as {mask_path}\")\n","        processed_list.append(filename)\n","      \n","  proc_images_n=len(set(processed_list))\n","  print(\"processing finished !!\")\n","  print(f\"{proc_images_n} images elaborated\")\n","  return set(processed_list)\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZSYQsRLu7v-a","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Cd71ShKn6R-RBXzhMaNYssjFCtxN_79p"},"executionInfo":{"status":"ok","timestamp":1605798426876,"user_tz":-60,"elapsed":45492,"user":{"displayName":"Matteo Jucker","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj9AZsDDD0OROwrJwgWwUjRWaTu83Jzgisj10GWag=s64","userId":"15343200840689388269"}},"outputId":"e7289d6e-cb73-4eb7-f085-671912027259"},"source":["detect_flowers_fruits(input_images, output_folder, model, show_predictions=True, save_predictions=False )"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"xzLjkXdUkoPs"},"source":["img_dir = os.path.join(PROJECT_DIR,INPUT_DIR_NAME)\n","gt_json_path= os.path.join(PROJECT_DIR,INPUT_ANNOTATIONS_FILE)\n"," \n"," # create dataset\n","dataset = HerbariaDataset()\n","dataset.load_data(gt_json_path, img_dir)\n","dataset.prepare()\n","for image_id in dataset.image_ids[:2]:\n","  # load image and annotations from GT and detections\n","  image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n","          modellib.load_image_gt(dataset, config,\n","                                  image_id)\n","  results = model.detect([image], verbose=0)\n","  r = results[0]\n","  # display gt\n","  visualize.display_instances(image, gt_bbox, gt_mask, gt_class_id, \n","                                  dataset.class_names,figsize=(100,100), show_mask=False)\n","  # display pred\n","  visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n","                                  dataset.class_names,figsize=(100,100), show_mask=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-ClvBop0CgiV"},"source":["# Model performance"]},{"cell_type":"code","metadata":{"id":"ftl8Bro4LacS"},"source":["# Compute VOC-style Average Precision\n","def compute_batch_ap(image_ids):\n","    sp_eval = {}\n","    sp_eval['AP']=[]\n","    sp_eval['precisions']=[]\n","    sp_eval['recalls']=[]\n","    sp_eval['overlaps']=[]\n","\n","\n","\n","    for image_id in image_ids:\n","        # Load image\n","        image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n","            modellib.load_image_gt(dataset, config,\n","                                   image_id)\n","        # Run object detection\n","        results = model.detect([image], verbose=0)\n","        # Compute AP\n","        r = results[0]\n","        AP, precisions, recalls, overlaps =\\\n","            utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n","                              r['rois'], r['class_ids'], r['scores'], r['masks'])\n","        sp_eval['AP'].append(AP)\n","        sp_eval['precisions'].append(precisions)\n","        sp_eval['recalls'].append(recalls)\n","        sp_eval['overlaps'].append(overlaps)\n","    return sp_eval\n","\n","# # Pick a set of random images\n","# image_ids = np.random.choice(dataset.image_ids, 10)\n","# APs = compute_batch_ap(image_ids)\n","# print(\"mAP @ IoU=50: \", np.mean(APs))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LvdB5GOjXFRt"},"source":["from utils import compute_matches"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EM95QJvpuJF7"},"source":["from sklearn.metrics import f1_score\n","import seaborn as sns\n","sns.color_palette(\"Spectral\", as_cmap=True)\n","def evaluate_model_performance(img_dir, gt_json_path, model, filter_score=0.5, viz=False):\n","  \"\"\" evaluates the model peroformance against a test set of images ad annotations\n","  Parameters:\n","  - model ??\n","  - detections folder with json files as retruned by <detect_flower_fruits> function\n","  - ground truth data (annotations) in COCO format\n","\n","  OPTIONAL Parameters:\n","  -plot outputs\n","  -masks\n","  \"\"\"\n","\n","\n","  # create dataset\n","  dataset = HerbariaDataset()\n","  dataset.load_data(gt_json_path, img_dir)\n","  dataset.prepare()\n","  \n","  # for det in os.listdir(detections_dir):\n","  #   if det.split(\".\") == \"json\":\n","  n_record=[]\n","  f1_boxes=[]\n","  recall_boxes=[]\n","  for image_id in dataset.image_ids:\n","  # load image and annotations from GT and detections\n","    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n","            modellib.load_image_gt(dataset, config,\n","                                   image_id)\n","    results = model.detect([image], verbose=0)\n","    r = results[0]\n","\n","    if filter_score:\n","        print(f\"removing predictions below {filter_score}\")\n","        sc_=r['scores']\n","        lim_=len(sc_[ sc_ > filter_score ])-1\n","        r['masks']=r['masks'][:,:,:lim_]\n","        r['rois']=r['rois'][:lim_]\n","        r['scores']= r['scores'][:lim_]\n","        r['class_ids'] = r['class_ids'][:lim_]\n","  # boxes classification\n","    max_len =min(len(gt_class_id), len(r['class_ids']))\n","    print(max_len)\n","    f1_=f1_score(gt_class_id[:max_len],r['class_ids'][:max_len])\n","    f1_boxes.append(f1_)\n","    print(\"\\n f1 score : \", f1_)\n","  \n","  # Overlaps boxes and masks\n","  # sp_eval = compute_batch_ap(dataset.image_ids)\n","  # IOU masks\n","  \n","    iou_recall,_=utils.compute_recall(r['rois'], gt_bbox, 0)\n","    print(\"recall boxes:\", iou_recall)\n","    recall_boxes.append(iou_recall)\n","  # break\n","  # compare areas\n","    # break\n","\n","\n","  print(f\" mean f1 score of batch : {np.mean(f1_boxes)}\")\n","  print(f\"mean recall score of batch : {np.mean(recall_boxes)}\")\n","  print(f1_boxes)\n","\n","  if viz:\n","    pass\n","    \n","    \n","  eval_results={\n","      \"f1_score\": f1_boxes, \n","      \"recall_boxes\": recall_boxes\n","                  \n","                  }\n","  return eval_results\n","\n","img_dir = os.path.join(PROJECT_DIR,INPUT_DIR_NAME)\n","gt_json_path= os.path.join(PROJECT_DIR,INPUT_ANNOTATIONS_FILE)\n","perf=evaluate_model_performance(img_dir, gt_json_path, model, 0.5, False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wL6AKRUhLctF"},"source":["import matplotlib\n","matplotlib.rc('axes',edgecolor=\"#FFFFFF\")\n","matplotlib.rc('lines', color= \"#FFFFFF\")\n","# matplotlib.rc('xticks', color= \"#FFFFFF\")\n","\n","\n","sns.color_palette(\"Spectral\", as_cmap=True)\n","fig, ax = plt.subplots(ncols=1, nrows=2, figsize=(20,20))\n","fig.set_facecolor(\"#000000\")\n","ax[0].hist(perf['f1_score'], bins=10, edgecolor='white', alpha=0.6)\n","# ax[0].set_facecolor(\"#000000\")\n","ax[0].set_title(\"F1 score object classification\")\n","ax[0].axvline(x=np.mean(perf['f1_score']), color=\"blue\")\n","# ax[0].plot(perf['f1_score'], color='blue', alpha=0.6)\n","\n","ax[1].hist(perf['recall_boxes'], bins=10, edgecolor='white', alpha=0.6)\n","ax[1].axvline(x=np.mean(perf['recall_boxes']), color=\"blue\")\n","\n","# ax[1].set_facecolor(\"#000000\")\n","ax[0].xaxis.label.set_color(\"#FFFFFF\")\n","ax[0].tick_params(axis='x', colors=\"#FFFFFF\")\n","ax[1].tick_params(axis='x', colors=\"#FFFFFF\")\n","ax[0].tick_params(axis='y', colors=\"#FFFFFF\")\n","ax[1].tick_params(axis='y', colors=\"#FFFFFF\")\n","ax[0].grid(axis='y',color=\"#000000\", lw=2)\n","ax[1].grid(axis='y',color=\"#000000\", lw=2)\n","\n","plt.savefig('eval1.pdf')  \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I4x-wx5UA1fM"},"source":["# Image analysis"]},{"cell_type":"code","metadata":{"id":"vxEV8jSgCMJk"},"source":["perf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wbBoBco2uU7q"},"source":["def analyse_batch_detections(image_folder, json_folder, mask_folder,):\n","  \"\"\"\" function to analys images AFTER detection. \n","  Returns dictionary with data about prediction batch  \n","  Parameters:\n","  json_folder:  folder with the output of the model as produced by <detect_flower_fruits> function\n","\n","  OPTIONAL parameters:\n","  mask_folder\n","  plot\n","  \"\"\"\n","  # load image and data\n","  \n","  # flowers\n","\n","  # image classification\n","\n","  # flowers per image mean and stdev\n","\n","  # fruits per image mean and standard dev\n","\n","  pass\n","\n","\n","\n"],"execution_count":null,"outputs":[]}]}